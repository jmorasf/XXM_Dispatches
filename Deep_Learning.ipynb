{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed value for the notebook so the results are reproducible\n",
    "from numpy.random import seed\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sklearn.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-25fb99ba413a3271",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17617, 6) (17617,)\n"
     ]
    }
   ],
   "source": [
    "# Use train_test_split to create training and testing data\n",
    "\n",
    "data = pd.read_csv('../Resources/XXM_DISPATCH4.csv')\n",
    "data.head()\n",
    "\n",
    "X = data[[\"MONTH\", \"DAY_OF_WEEK\", \"HOUR\",\"DIFF\", \"MAX_TEMPERATURE\", \"INCREASED_UNITS\"]]\n",
    "y = data[\"DISPATCH_ONE\"]\n",
    "\n",
    "print(X.shape, y.shape) \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7e1de5d9b7942f68",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a StandardScater model and fit it to the training data\n",
    "X_scaler = StandardScaler().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9532e12246e485d5",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Transform the training and testing data using the X_scaler\n",
    "\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "y_train_categorical = to_categorical(y_train)\n",
    "y_test_categorical = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ec399a95e133cb58",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# first, create a normal neural network with 2 inputs, 6 hidden nodes, and 2 outputs\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=6, activation='relu', input_dim=6))\n",
    "model.add(Dense(units=2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 6)                 42        \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 2)                 14        \n",
      "=================================================================\n",
      "Total params: 56\n",
      "Trainable params: 56\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5cf2fbdbea0ed50b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13212 samples\n",
      "Epoch 1/10\n",
      "13212/13212 - 1s - loss: 0.5166 - accuracy: 0.7445\n",
      "Epoch 2/10\n",
      "13212/13212 - 1s - loss: 0.2543 - accuracy: 0.9124\n",
      "Epoch 3/10\n",
      "13212/13212 - 1s - loss: 0.1722 - accuracy: 0.9391\n",
      "Epoch 4/10\n",
      "13212/13212 - 1s - loss: 0.1390 - accuracy: 0.9466\n",
      "Epoch 5/10\n",
      "13212/13212 - 1s - loss: 0.1224 - accuracy: 0.9516\n",
      "Epoch 6/10\n",
      "13212/13212 - 1s - loss: 0.1127 - accuracy: 0.9559\n",
      "Epoch 7/10\n",
      "13212/13212 - 1s - loss: 0.1065 - accuracy: 0.9578\n",
      "Epoch 8/10\n",
      "13212/13212 - 1s - loss: 0.1024 - accuracy: 0.9596\n",
      "Epoch 9/10\n",
      "13212/13212 - 0s - loss: 0.0996 - accuracy: 0.9618\n",
      "Epoch 10/10\n",
      "13212/13212 - 0s - loss: 0.0976 - accuracy: 0.9639\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20aa07a6c88>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model to the training data\n",
    "model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_categorical,\n",
    "    epochs=10,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "For this network, we simply add an additional hidden layer of 6 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_model = Sequential()\n",
    "deep_model.add(Dense(units=6, activation='relu', input_dim=6))\n",
    "deep_model.add(Dense(units=6, activation='softmax'))\n",
    "deep_model.add(Dense(units=6, activation='relu'))\n",
    "deep_model.add(Dense(units=2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_14 (Dense)             (None, 6)                 42        \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 6)                 42        \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 6)                 42        \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 2)                 14        \n",
      "=================================================================\n",
      "Total params: 140\n",
      "Trainable params: 140\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "deep_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13212 samples\n",
      "Epoch 1/10\n",
      "13212/13212 - 1s - loss: 0.4573 - accuracy: 0.8176\n",
      "Epoch 2/10\n",
      "13212/13212 - 1s - loss: 0.2400 - accuracy: 0.8506\n",
      "Epoch 3/10\n",
      "13212/13212 - 1s - loss: 0.1774 - accuracy: 0.9556\n",
      "Epoch 4/10\n",
      "13212/13212 - 0s - loss: 0.1491 - accuracy: 0.9615\n",
      "Epoch 5/10\n",
      "13212/13212 - 0s - loss: 0.1266 - accuracy: 0.9602\n",
      "Epoch 6/10\n",
      "13212/13212 - 0s - loss: 0.1050 - accuracy: 0.9607\n",
      "Epoch 7/10\n",
      "13212/13212 - 1s - loss: 0.1005 - accuracy: 0.9615\n",
      "Epoch 8/10\n",
      "13212/13212 - 0s - loss: 0.0987 - accuracy: 0.9620\n",
      "Epoch 9/10\n",
      "13212/13212 - 1s - loss: 0.0972 - accuracy: 0.9631\n",
      "Epoch 10/10\n",
      "13212/13212 - 1s - loss: 0.0957 - accuracy: 0.9640\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20aa2bfdf98>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_model.compile(optimizer='adam',\n",
    "                   loss='categorical_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "deep_model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_categorical,\n",
    "    epochs=10,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the models below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4405/1 - 0s - loss: nan - accuracy: 0.9600\n",
      "Normal Neural Network - Loss: nan, Accuracy: 0.9600453972816467\n"
     ]
    }
   ],
   "source": [
    "model_loss, model_accuracy = model.evaluate(\n",
    "    X_test_scaled, y_test_categorical, verbose=2)\n",
    "print(\n",
    "    f\"Normal Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4405/1 - 0s - loss: nan - accuracy: 0.9614\n",
      "Deep Neural Network - Loss: nan, Accuracy: 0.961407482624054\n"
     ]
    }
   ],
   "source": [
    "model_loss, model_accuracy = deep_model.evaluate(\n",
    "    X_test_scaled, y_test_categorical, verbose=2)\n",
    "print(f\"Deep Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.06677869 -0.44309111  0.50389128  1.39293414  2.68723313 -0.49803961]\n",
      "[-1.17067499  0.48488222 -0.94280601  0.48696762 -0.78467418 -0.02582322]\n",
      "[ 1.    5.   17.    1.95 59.   13.  ]\n"
     ]
    }
   ],
   "source": [
    "# Validating the type of data\n",
    "print(X_test_scaled[10, :])\n",
    "print(X_train_scaled[10, :])\n",
    "print(X.to_numpy()[10, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98      3630\n",
      "           1       0.91      0.87      0.89       775\n",
      "\n",
      "    accuracy                           0.96      4405\n",
      "   macro avg       0.94      0.93      0.93      4405\n",
      "weighted avg       0.96      0.96      0.96      4405\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate classification report\n",
    "from sklearn.metrics import classification_report\n",
    "predictions = deep_model.predict_classes(X_test_scaled)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  1 - Day of Week:  5 - Hour:  9.00 - Proj. Demand/Available: -2.45 - Trend: -1.00 - Below Projected Demand\n",
      "i:  11 - Day of Week:  5 - Hour:  5.00 - Proj. Demand/Available:  3.10 - Trend:  0.00 - Enough Ambulances\n",
      "i:  21 - Day of Week:  5 - Hour:  1.00 - Proj. Demand/Available:  8.47 - Trend:  6.00 - Enough Ambulances\n",
      "i:  31 - Day of Week:  7 - Hour: 19.00 - Proj. Demand/Available: -0.57 - Trend: -2.00 - Below Projected Demand\n",
      "i:  41 - Day of Week:  7 - Hour:  6.00 - Proj. Demand/Available:  2.78 - Trend:  0.00 - Enough Ambulances\n",
      "i:  51 - Day of Week:  1 - Hour: 18.00 - Proj. Demand/Available: -2.03 - Trend:  0.00 - Below Projected Demand\n",
      "i:  61 - Day of Week:  1 - Hour: 21.00 - Proj. Demand/Available:  1.20 - Trend: -2.00 - Enough Ambulances\n",
      "i:  71 - Day of Week:  1 - Hour:  1.00 - Proj. Demand/Available:  3.56 - Trend: -2.00 - Enough Ambulances\n",
      "i:  81 - Day of Week:  2 - Hour: 19.00 - Proj. Demand/Available: -1.26 - Trend:  1.00 - Below Projected Demand\n",
      "i:  91 - Day of Week:  2 - Hour:  2.00 - Proj. Demand/Available:  3.59 - Trend:  1.00 - Enough Ambulances\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 100, 10):\n",
    "    Xnew = data.loc[[i], [\"MONTH\", \"DAY_OF_WEEK\", \"HOUR\",\"DIFF\", \"MAX_TEMPERATURE\",\"INCREASED_UNITS\"]]\n",
    "    Xnew_Scaled= X_scaler.transform(Xnew)\n",
    "\n",
    "    res = deep_model.predict_classes(Xnew_Scaled)\n",
    "\n",
    "    if  res.item((0)) == 1:\n",
    "        prediction = 'Below Projected Demand'\n",
    "    else:\n",
    "        prediction = 'Enough Ambulances'\n",
    "        \n",
    "    print(\"i: % 2d - Day of Week: % 2d - Hour: %5.2f - Proj. Demand/Available: %5.2f - Trend: %5.2f - %s\" \\\n",
    "          % (i, Xnew.iloc[0]['DAY_OF_WEEK'], Xnew.iloc[0]['HOUR'], Xnew.iloc[0]['DIFF'],Xnew.iloc[0]['INCREASED_UNITS'],prediction))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
